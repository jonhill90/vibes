# Source: repos/codex/docs/config.md + docs/config.toml MCP server patterns
# Pattern: Codex profile configuration with MCP servers
# Extracted: 2025-10-07
# Relevance: 10/10 - Essential config template for Codex integration

# =============================================================================
# CODEX PROFILE: codex-prp
# =============================================================================
# Purpose: Dedicated profile for PRP generation/execution via Codex CLI
# Location: ~/.codex/config.toml (user-level) OR .codex/config.toml (repo-local)
#
# What to Mimic:
#   - Profile structure with [profiles.<name>] syntax
#   - Explicit model, approval, sandbox settings (no defaults)
#   - MCP server configuration with command + args pattern
#   - Timeout settings for long-running operations
#
# What to Adapt:
#   - Model selection (o4-mini, gpt-5-codex, o3) based on use case
#   - Approval policy (on-request, on-failure, never) per workflow
#   - MCP servers (add/remove based on needs)
#   - Working directory (cwd) to match your repo path
#
# What to Skip:
#   - Azure/Ollama provider config (unless you need them)
#   - HTTP headers (only for custom providers)

[profiles.codex-prp]
# Model Configuration
# - o4-mini: Balanced reasoning/speed for PRP generation
# - gpt-5-codex: Fast iteration for execution
# - o3: Deep analysis for complex gotcha detection
model = "o4-mini"

# Approval Policy
# - "untrusted": Prompt for all commands (safest, slowest)
# - "on-request": Prompt before tool use (recommended for generation)
# - "on-failure": Only prompt on errors (recommended for execution)
# - "never": Full automation (use with caution)
approval_policy = "on-request"

# Sandbox Mode
# - "read-only": No writes, no network (default for codex exec)
# - "workspace-write": Allow writes to workspace roots only
# - "danger-full-access": Full system access (requires explicit flag)
sandbox_mode = "workspace-write"

# Timeout Configuration (in seconds)
# - startup_timeout_sec: How long to wait for MCP servers to start
# - tool_timeout_sec: Max execution time for tools (10 min for complex phases)
startup_timeout_sec = 30
tool_timeout_sec = 600  # 10 minutes

# Working Directory
# Set to repo root for consistent file paths
cwd = "/Users/jon/source/vibes"

# =============================================================================
# MCP Servers (STDIO and HTTP patterns)
# =============================================================================
# Pattern: Each server has either:
#   - STDIO: command + args + optional env
#   - HTTP: url (for HTTP-based servers like Archon)

# Archon MCP Server (HTTP)
[profiles.codex-prp.mcp_servers.archon]
url = "http://localhost:8051/mcp"
# Note: HTTP servers don't use command/args, only URL

# Basic Memory MCP Server (STDIO via Docker)
[profiles.codex-prp.mcp_servers.basic_memory]
command = "docker"
args = ["exec", "-i", "basic-memory-mcp", "/app/start.sh"]

# Vibesbox MCP Server (STDIO via Docker)
[profiles.codex-prp.mcp_servers.vibesbox]
command = "docker"
args = ["exec", "-i", "mcp-vibesbox-server", "python3", "/workspace/server.py"]

# Docker MCP Server (STDIO)
[profiles.codex-prp.mcp_servers.docker_mcp]
command = "docker"
args = ["mcp", "gateway", "run"]

# =============================================================================
# Alternative: Archon as STDIO Server (if uvx installation available)
# =============================================================================
# Uncomment if you want to run Archon as STDIO instead of HTTP
# [profiles.codex-prp.mcp_servers.archon_stdio]
# command = "uvx"
# args = ["archon"]
# env = { ARCHON_ENV = "production" }

# =============================================================================
# Model Provider Overrides (Optional)
# =============================================================================
# Uncomment to add custom providers (Ollama, Azure, etc.)

# [model_providers.ollama]
# name = "Ollama Local"
# base_url = "http://localhost:11434/v1"
# wire_api = "chat"

# [model_providers.azure]
# name = "Azure OpenAI"
# base_url = "https://YOUR_PROJECT.openai.azure.com/openai"
# env_key = "AZURE_OPENAI_API_KEY"
# query_params = { api-version = "2025-04-01-preview" }
# wire_api = "responses"

# =============================================================================
# Network Tuning (Per-Provider, Optional)
# =============================================================================
# Set inside [model_providers.<id>] block for retry/timeout control
# Example:
# [model_providers.openai]
# request_max_retries = 4            # Retry failed requests
# stream_max_retries = 10            # Retry dropped streams
# stream_idle_timeout_ms = 300000    # 5 minute idle timeout

# =============================================================================
# Usage Instructions
# =============================================================================
# 1. Copy this to ~/.codex/config.toml (user-level)
#    OR .codex/config.toml (repo-local for team sharing)
#
# 2. Update `cwd` to match your repository path
#
# 3. Test profile:
#    codex config show --profile codex-prp
#    codex config validate
#
# 4. Use profile with exec:
#    codex exec --profile codex-prp --prompt "task description"
#
# 5. Verify MCP servers:
#    Check logs at ~/.codex/logs/ for server startup issues
