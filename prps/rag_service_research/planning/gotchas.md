# Known Gotchas: RAG Service Research

## Overview

This document identifies critical issues, common mistakes, security vulnerabilities, and performance pitfalls when building a RAG (Retrieval Augmented Generation) service. All gotchas include **concrete solutions** with code examples to avoid or fix them. Research covers vector databases (Qdrant, pgvector), embedding APIs (OpenAI), PostgreSQL hybrid search, async patterns, document processing, and chunking strategies.

**Severity Levels**:
- **Critical**: Security vulnerabilities, data loss risks, system crashes
- **High**: Common bugs, performance degradation, integration failures
- **Medium**: API quirks, confusing behavior, inefficient patterns
- **Low**: Minor annoyances, style issues

---

## Critical Gotchas

### 1. OpenAI API Quota Exhaustion (Data Loss Risk)

**Severity**: Critical
**Category**: Embedding Generation / Data Loss
**Affects**: OpenAI Embeddings API (text-embedding-3-small/large)
**Source**: https://community.openai.com/t/rate-limit-reached-with-large-documents/358525

**What it is**:
When you exceed your OpenAI API quota during batch embedding generation, the API returns a 429 error with "insufficient_quota". If you continue storing documents without embeddings, you corrupt your vector database with null or zero embeddings, making them impossible to retrieve.

**Why it's a problem**:
- Corrupts search results (zero vectors match everything with equal irrelevance)
- Silent data loss - documents ingested but not searchable
- No way to identify which documents failed without tracking

**How to detect it**:
- Error message: `"Error code: 429 - {'error': {'message': 'You exceeded your current quota...'}}"`
- Monitor for `RateLimitError` with "insufficient_quota" in message
- Test: Query for documents with null embeddings in vector DB

**How to avoid/fix**:

```python
# ‚ùå WRONG - Stores corrupted data on quota exhaustion
async def ingest_documents(documents: list[Document]):
    for doc in documents:
        try:
            embedding = await openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=doc.text
            )
            vector = embedding.data[0].embedding
        except Exception:
            vector = [0.0] * 1536  # üíÄ CORRUPTS SEARCH!

        await qdrant.upsert(vector=vector)  # Stores garbage

# ‚úÖ RIGHT - Skip failed items, track for retry
from dataclasses import dataclass, field

@dataclass
class EmbeddingBatchResult:
    embeddings: list[list[float]] = field(default_factory=list)
    failed_items: list[dict] = field(default_factory=list)
    success_count: int = 0
    failure_count: int = 0

    def add_failure(self, text: str, error: Exception):
        self.failed_items.append({
            "text": text[:200],
            "error": str(error),
            "error_type": type(error).__name__
        })
        self.failure_count += 1

async def ingest_documents_safe(documents: list[Document]) -> EmbeddingBatchResult:
    result = EmbeddingBatchResult()

    for doc in documents:
        try:
            response = await openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=doc.text
            )
            embedding = response.data[0].embedding

            # Only store if embedding generation succeeded
            await qdrant.upsert(vector=embedding, payload={"doc_id": doc.id})
            result.success_count += 1

        except openai.RateLimitError as e:
            if "insufficient_quota" in str(e):
                # STOP EVERYTHING - quota exhausted
                logger.error(f"Quota exhausted after {result.success_count} documents")
                # Mark remaining documents as failed
                for remaining_doc in documents[result.success_count:]:
                    result.add_failure(remaining_doc.text, e)
                break
            else:
                # Regular rate limit - will retry
                result.add_failure(doc.text, e)
        except Exception as e:
            result.add_failure(doc.text, e)

    # Log failures for manual review/retry
    if result.failed_items:
        logger.error(f"Failed to embed {result.failure_count} documents")
        await save_failed_items_for_retry(result.failed_items)

    return result

# Why this works:
# 1. Stops immediately on quota exhaustion (prevents further corruption)
# 2. Only stores documents with valid embeddings
# 3. Tracks failures for manual retry after quota refill
# 4. Provides visibility into what failed
```

**Additional Resources**:
- OpenAI Rate Limits Guide: https://platform.openai.com/docs/guides/rate-limits
- Rate Limit Handling Cookbook: https://cookbook.openai.com/examples/how_to_handle_rate_limits

---

### 2. asyncpg Connection Pool Deadlock

**Severity**: Critical
**Category**: System Stability / Connection Management
**Affects**: FastAPI + asyncpg connection pooling
**Source**: https://github.com/nsidnev/fastapi-realworld-example-app/issues/64

**What it is**:
When using FastAPI dependencies that each acquire a connection from the pool, you can exhaust the pool under concurrent load. Once exhausted, requests wait for connections that never become available (held by other waiting requests), causing a complete application deadlock.

**Why it's a problem**:
- Total application freeze - all requests hang indefinitely
- Requires service restart to recover
- Happens unpredictably under load (race condition)
- Hard to debug - no error, just hanging requests

**How to detect it**:
- Symptoms: All API endpoints stop responding, connections stuck in "idle in transaction"
- PostgreSQL: `SELECT * FROM pg_stat_activity WHERE state = 'idle in transaction';` shows many connections
- asyncpg: Pool acquisition times out after 60s (command_timeout)
- Load test: 20+ concurrent requests cause deadlock with pool size 10

**How to avoid/fix**:

```python
# ‚ùå WRONG - Each dependency acquires a connection (DEADLOCKS!)
from fastapi import Depends

async def get_db_connection(pool=Depends(get_pool)):
    async with pool.acquire() as conn:
        yield conn  # üíÄ Held until request completes

@app.get("/documents/{doc_id}")
async def get_document(
    doc_id: str,
    # If you have 3 dependencies, you need 3 pool connections per request!
    conn1=Depends(get_db_connection),  # Acquires connection 1
    conn2=Depends(get_db_connection),  # Acquires connection 2
    conn3=Depends(get_db_connection)   # Acquires connection 3
):
    # With pool size 10, only 3-4 concurrent requests possible
    # Request 4 waits for connection, blocking pool
    pass

# ‚úÖ RIGHT - Share the pool, acquire connections as needed
from contextlib import asynccontextmanager
from fastapi import Request

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Create pool once at startup
    app.state.db_pool = await asyncpg.create_pool(
        host="localhost",
        database="rag_service",
        min_size=10,
        max_size=20,
        max_inactive_connection_lifetime=300.0,
        command_timeout=60.0
    )
    yield
    await app.state.db_pool.close()

app = FastAPI(lifespan=lifespan)

# Dependency returns POOL, not connection
async def get_pool(request: Request) -> asyncpg.Pool:
    return request.app.state.db_pool

# Service layer manages connections internally
class DocumentService:
    def __init__(self, db_pool: asyncpg.Pool):
        self.db_pool = db_pool

    async def get_document(self, doc_id: str) -> tuple[bool, dict]:
        # Acquire connection only when needed, release immediately
        async with self.db_pool.acquire() as conn:
            row = await conn.fetchrow(
                "SELECT * FROM documents WHERE id = $1",
                doc_id
            )
            if not row:
                return False, {"error": "Document not found"}
            return True, {"document": dict(row)}

@app.get("/documents/{doc_id}")
async def get_document_route(
    doc_id: str,
    pool: asyncpg.Pool = Depends(get_pool)  # Just pass pool
):
    service = DocumentService(pool)
    success, result = await service.get_document(doc_id)
    if not success:
        raise HTTPException(status_code=404, detail=result)
    return result

# Why this works:
# 1. Pool is shared across all routes (never exhausted)
# 2. Connections acquired only when executing queries
# 3. Connections released immediately after query completes
# 4. Single connection per request regardless of dependencies
```

**Pool Sizing Guidelines**:
```python
# For async workloads (FastAPI):
# min_size = number of CPU cores (e.g., 10 for 8-core machine)
# max_size = 2-3x CPU cores (e.g., 20-30)
# Avoid: max_size > PostgreSQL max_connections (default 100)

pool = await asyncpg.create_pool(
    min_size=10,  # Always maintain 10 connections
    max_size=20,  # Max 20 concurrent connections
    max_inactive_connection_lifetime=300.0,  # Recycle after 5 min
    command_timeout=60.0  # Kill slow queries after 60s
)
```

**Additional Resources**:
- FastAPI Connection Pooling Guide: https://usamabjw.medium.com/improving-latency-of-database-calls-in-fastapi-with-asyncio-lifespan-events-and-connection-818066db59ab

---

### 3. asyncpg Placeholder Syntax Error (SQL Injection Risk)

**Severity**: Critical
**Category**: Security / SQL Injection
**Affects**: asyncpg (not psycopg2/psycopg3)
**Source**: Task-manager gotcha #7 from codebase-patterns.md

**What it is**:
asyncpg uses `$1, $2` placeholders for parameterized queries, NOT `%s` (psycopg2 style). Using `%s` with asyncpg fails silently or causes SQL injection vulnerabilities if you manually format strings.

**Why it's a problem**:
- SQL injection vulnerability if you resort to string formatting
- Runtime errors: `asyncpg.exceptions.SyntaxError: unexpected '%'`
- Code from psycopg2 examples won't work with asyncpg
- Easy to miss in code review (looks normal if you're used to psycopg2)

**How to detect it**:
- Error: `asyncpg.exceptions.SyntaxError: syntax error at or near "%"`
- Symptom: Query works in PostgreSQL directly but fails via asyncpg
- Test: Check if query string contains `%s` instead of `$1, $2`

**How to avoid/fix**:

```python
# ‚ùå WRONG - psycopg2 syntax (doesn't work with asyncpg)
async with pool.acquire() as conn:
    # This will cause a syntax error!
    await conn.fetch(
        "SELECT * FROM documents WHERE id = %s AND status = %s",
        doc_id, status
    )

    # Even worse - manual string formatting (SQL INJECTION!)
    query = f"SELECT * FROM documents WHERE title = '{user_input}'"
    # If user_input = "'; DROP TABLE documents; --"
    # Result: Your data is gone! üíÄ
    await conn.fetch(query)

# ‚úÖ RIGHT - asyncpg syntax with positional parameters
async with pool.acquire() as conn:
    # Use $1, $2, $3 for parameters
    rows = await conn.fetch(
        "SELECT * FROM documents WHERE id = $1 AND status = $2",
        doc_id,  # $1
        status   # $2
    )

    # Multiple columns
    await conn.execute(
        "INSERT INTO documents (id, title, content, source_id) VALUES ($1, $2, $3, $4)",
        doc_id, title, content, source_id
    )

    # IN clause requires ANY($1::uuid[])
    document_ids = ["uuid1", "uuid2", "uuid3"]
    rows = await conn.fetch(
        "SELECT * FROM documents WHERE id = ANY($1::uuid[])",
        document_ids
    )

# Why this works:
# 1. asyncpg properly escapes parameters (prevents SQL injection)
# 2. Positional placeholders are clear and explicit
# 3. Type safety - asyncpg validates types match column types
```

**Quick Reference Table**:

| Operation | psycopg2 (WRONG) | asyncpg (RIGHT) |
|-----------|------------------|-----------------|
| Single param | `WHERE id = %s` | `WHERE id = $1` |
| Multiple params | `VALUES (%s, %s, %s)` | `VALUES ($1, $2, $3)` |
| IN clause | `WHERE id IN %s` | `WHERE id = ANY($1::type[])` |
| LIKE pattern | `WHERE title LIKE %s` | `WHERE title LIKE $1` |

**Additional Resources**:
- asyncpg documentation: https://magicstack.github.io/asyncpg/current/usage.html

---

### 4. PostgreSQL Row Locking Without ORDER BY (Deadlock)

**Severity**: Critical
**Category**: Concurrency / Database Deadlocks
**Affects**: PostgreSQL transactions with SELECT FOR UPDATE
**Source**: Task-manager gotcha #2 from codebase-patterns.md

**What it is**:
When multiple concurrent transactions lock rows using `SELECT ... FOR UPDATE` without consistent ordering, PostgreSQL can deadlock. Transaction A locks row 1 then waits for row 2, while Transaction B locks row 2 then waits for row 1.

**Why it's a problem**:
- Random application crashes under concurrent load
- Unpredictable - works fine in testing, fails in production
- PostgreSQL automatically kills one transaction (error 40P01)
- User data loss if transaction is rolled back
- Very hard to reproduce and debug

**How to detect it**:
- Error: `asyncpg.exceptions.DeadlockDetectedError: deadlock detected`
- PostgreSQL log: `DETAIL: Process 1234 waits for ShareLock on transaction 5678`
- Symptom: Occasional transaction failures under high concurrency
- Test: Run 10+ concurrent updates on same resource

**How to avoid/fix**:

```python
# ‚ùå WRONG - No ORDER BY = potential deadlock
async def reorder_chunks(document_id: str, new_positions: dict):
    async with pool.acquire() as conn:
        async with conn.transaction():
            # üíÄ DEADLOCK RISK - locks in arbitrary order
            await conn.execute(
                """
                SELECT id FROM chunks
                WHERE document_id = $1
                FOR UPDATE
                """,
                document_id
            )

            # If two transactions lock different chunks first,
            # they deadlock when trying to lock remaining chunks
            for chunk_id, position in new_positions.items():
                await conn.execute(
                    "UPDATE chunks SET position = $1 WHERE id = $2",
                    position, chunk_id
                )

# ‚úÖ RIGHT - ORDER BY id ensures consistent lock order
async def reorder_chunks_safe(document_id: str, new_positions: dict):
    async with pool.acquire() as conn:
        async with conn.transaction():
            # Lock rows in consistent order (by id)
            # All transactions lock in same order = no deadlock
            rows = await conn.fetch(
                """
                SELECT id, position FROM chunks
                WHERE document_id = $1
                ORDER BY id  -- CRITICAL: Prevents deadlock
                FOR UPDATE
                """,
                document_id
            )

            # Now safe to update in any order
            for chunk_id, position in new_positions.items():
                await conn.execute(
                    "UPDATE chunks SET position = $1 WHERE id = $2",
                    position, chunk_id
                )

# Example: Moving chunks to new positions
async def move_chunk_position(
    chunk_id: str,
    document_id: str,
    new_position: int
) -> tuple[bool, dict]:
    """Move a chunk to a new position, shifting others."""
    async with pool.acquire() as conn:
        async with conn.transaction():
            # Step 1: Lock all affected chunks in consistent order
            await conn.execute(
                """
                SELECT id FROM chunks
                WHERE document_id = $1 AND position >= $2
                ORDER BY id  -- Prevents deadlock
                FOR UPDATE
                """,
                document_id,
                new_position
            )

            # Step 2: Shift positions (batch update)
            await conn.execute(
                """
                UPDATE chunks
                SET position = position + 1, updated_at = NOW()
                WHERE document_id = $1 AND position >= $2
                """,
                document_id,
                new_position
            )

            # Step 3: Move target chunk
            await conn.execute(
                """
                UPDATE chunks
                SET position = $1, updated_at = NOW()
                WHERE id = $2
                """,
                new_position,
                chunk_id
            )

    return True, {"message": "Chunk moved successfully"}

# Why this works:
# 1. ORDER BY id ensures all transactions lock rows in same order
# 2. PostgreSQL can't deadlock if lock acquisition order is consistent
# 3. id is indexed, so ORDER BY is fast
# 4. Works even with thousands of concurrent transactions
```

**Deadlock Prevention Checklist**:
- [ ] All `SELECT ... FOR UPDATE` queries include `ORDER BY id`
- [ ] Multi-step transactions lock ALL needed rows upfront
- [ ] Lock ordering is consistent across entire codebase
- [ ] Use batch updates instead of row-by-row updates where possible

**Additional Resources**:
- PostgreSQL Deadlock Documentation: https://www.postgresql.org/docs/current/explicit-locking.html

---

## High Priority Gotchas

### 5. Qdrant Collection Dimension Mismatch

**Severity**: High
**Category**: Configuration / Data Loss
**Affects**: Qdrant vector database
**Source**: https://qdrant.tech/documentation/

**What it is**:
Qdrant collections are created with a fixed vector dimension (e.g., 1536 for OpenAI text-embedding-3-small). If you try to insert vectors with different dimensions, Qdrant rejects them with a validation error. If you change embedding models without recreating collections, all new data is lost.

**Why it's a problem**:
- Silent data loss - inserts fail but app continues
- Must recreate collection to change dimensions (deletes all data)
- No automatic migration path
- Easy to misconfigure during model changes

**How to detect it**:
- Error: `ValueError: Vector dimension mismatch: expected 1536, got 768`
- Symptom: Document ingestion succeeds but search returns no results
- Test: Query collection config and compare to embedding dimension

**How to avoid/fix**:

```python
# ‚ùå WRONG - Hardcoded dimensions, no validation
from qdrant_client import AsyncQdrantClient
from qdrant_client.models import Distance, VectorParams

async def create_collection():
    client = AsyncQdrantClient("localhost")
    await client.create_collection(
        collection_name="documents",
        vectors_config=VectorParams(
            size=1536,  # üíÄ Hardcoded - breaks if you change models
            distance=Distance.COSINE
        )
    )

# ‚úÖ RIGHT - Dimension from config, validation before insert
from typing import Literal
from pydantic import BaseModel

class EmbeddingConfig(BaseModel):
    provider: Literal["openai", "sentence_transformers"]
    model_name: str
    dimension: int

    @classmethod
    def from_model_name(cls, provider: str, model: str) -> "EmbeddingConfig":
        # Centralized dimension mapping
        configs = {
            ("openai", "text-embedding-3-small"): 1536,
            ("openai", "text-embedding-3-large"): 3072,
            ("sentence_transformers", "all-MiniLM-L6-v2"): 384,
            ("sentence_transformers", "all-mpnet-base-v2"): 768,
        }
        dimension = configs.get((provider, model))
        if not dimension:
            raise ValueError(f"Unknown embedding model: {provider}/{model}")

        return cls(provider=provider, model_name=model, dimension=dimension)

class VectorService:
    def __init__(self, embedding_config: EmbeddingConfig):
        self.config = embedding_config
        self.client = AsyncQdrantClient("localhost")

    async def ensure_collection(self, collection_name: str):
        """Create collection with correct dimensions or validate existing."""
        try:
            # Check if collection exists
            collection_info = await self.client.get_collection(collection_name)

            # Validate dimensions match
            existing_dim = collection_info.config.params.vectors.size
            if existing_dim != self.config.dimension:
                raise ValueError(
                    f"Collection '{collection_name}' has dimension {existing_dim}, "
                    f"but embedding model '{self.config.model_name}' produces {self.config.dimension}. "
                    f"Either recreate the collection or use matching embedding model."
                )

        except Exception:
            # Collection doesn't exist - create it
            await self.client.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(
                    size=self.config.dimension,
                    distance=Distance.COSINE
                )
            )

    async def insert_vectors(
        self,
        collection_name: str,
        vectors: list[list[float]],
        payloads: list[dict]
    ):
        """Insert vectors with dimension validation."""
        # Validate dimensions before inserting
        for i, vector in enumerate(vectors):
            if len(vector) != self.config.dimension:
                raise ValueError(
                    f"Vector {i} has dimension {len(vector)}, "
                    f"expected {self.config.dimension}"
                )

        # Safe to insert
        await self.client.upsert(
            collection_name=collection_name,
            points=[
                {"id": i, "vector": vec, "payload": payload}
                for i, (vec, payload) in enumerate(zip(vectors, payloads))
            ]
        )

# Usage with environment-driven config
import os

embedding_config = EmbeddingConfig.from_model_name(
    provider=os.getenv("EMBEDDING_PROVIDER", "openai"),
    model=os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
)

vector_service = VectorService(embedding_config)

# Why this works:
# 1. Dimensions derived from model name (single source of truth)
# 2. Validates collection dimensions match current config
# 3. Clear error messages guide fix (recreate collection or change model)
# 4. Prevents silent data loss
```

**Migration Strategy** (if changing models):

```python
async def migrate_collection_to_new_model(
    old_collection: str,
    new_collection: str,
    old_service: VectorService,
    new_service: VectorService
):
    """Migrate vectors to new embedding model."""
    # 1. Retrieve all documents with text
    documents = await get_all_documents_with_text()

    # 2. Create new collection with new dimensions
    await new_service.ensure_collection(new_collection)

    # 3. Re-embed and insert
    batch_size = 100
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i+batch_size]

        # Generate embeddings with NEW model
        new_embeddings = await generate_embeddings([d.text for d in batch])

        # Insert to new collection
        await new_service.insert_vectors(
            collection_name=new_collection,
            vectors=new_embeddings,
            payloads=[{"doc_id": d.id, "text": d.text} for d in batch]
        )

    # 4. Swap collections (update app config)
    # 5. Delete old collection
    await old_service.client.delete_collection(old_collection)
```

---

### 6. pgvector Index Not Used (Full Table Scan)

**Severity**: High
**Category**: Performance / Query Optimization
**Affects**: PostgreSQL pgvector with HNSW/IVFFlat indexes
**Source**: https://github.com/pgvector/pgvector/issues/455

**What it is**:
PostgreSQL query planner may ignore your pgvector index and do full table scans if the query is too complex, uses wrong operators, or includes conditions that prevent index usage. This makes queries 100-1000x slower.

**Why it's a problem**:
- Query times go from 10ms to 10+ seconds
- CPU and disk I/O spike
- Scales terribly (linear with table size)
- Hard to notice until production load

**How to detect it**:
- Use `EXPLAIN ANALYZE` and look for "Seq Scan" instead of "Index Scan"
- Symptom: Vector searches taking seconds on 100K+ rows
- Test: Query time increases linearly with table size

**How to avoid/fix**:

```sql
-- ‚ùå WRONG - Index not used (complex query)
EXPLAIN ANALYZE
SELECT id, content, embedding <=> '[0.1, 0.2, ...]'::vector AS distance
FROM document_chunks
WHERE source_id = 'some-uuid'
  AND deleted_at IS NULL
  AND (metadata->>'type' = 'paragraph')
ORDER BY embedding <=> '[0.1, 0.2, ...]'::vector
LIMIT 10;

-- Result: Seq Scan on document_chunks (BAD!)
-- Planning Time: 0.5 ms
-- Execution Time: 8743.2 ms (full table scan)

-- ‚úÖ RIGHT - Simplify query to use index
-- Option 1: Use index for vector search, filter afterwards
EXPLAIN ANALYZE
WITH vector_results AS (
  SELECT id, content, source_id, deleted_at, metadata,
         embedding <=> '[0.1, 0.2, ...]'::vector AS distance
  FROM document_chunks
  ORDER BY embedding <=> '[0.1, 0.2, ...]'::vector
  LIMIT 100  -- Get top 100 from index
)
SELECT id, content, distance
FROM vector_results
WHERE source_id = 'some-uuid'
  AND deleted_at IS NULL
  AND (metadata->>'type' = 'paragraph')
ORDER BY distance
LIMIT 10;

-- Result: Index Scan using chunks_embedding_idx (GOOD!)
-- Planning Time: 0.3 ms
-- Execution Time: 12.4 ms (700x faster!)

-- Option 2: Create partial index for common filters
CREATE INDEX chunks_embedding_active_idx
ON document_chunks
USING hnsw (embedding vector_cosine_ops)
WHERE deleted_at IS NULL;

-- Then query can filter directly
SELECT id, content, embedding <=> '[0.1, 0.2, ...]'::vector AS distance
FROM document_chunks
WHERE deleted_at IS NULL  -- Matches partial index condition
ORDER BY embedding <=> '[0.1, 0.2, ...]'::vector
LIMIT 10;
```

**Index Creation Best Practices**:

```sql
-- Distance operator MUST match index type
-- Wrong: Index uses cosine, query uses L2
CREATE INDEX chunks_embedding_idx
ON document_chunks
USING hnsw (embedding vector_cosine_ops);  -- <=> operator

SELECT * FROM document_chunks
ORDER BY embedding <-> '[...]'::vector;  -- üíÄ Uses <-> (L2), not <=> (cosine)

-- Right: Match operators
-- For cosine similarity (recommended for text):
CREATE INDEX chunks_embedding_cosine_idx
ON document_chunks
USING hnsw (embedding vector_cosine_ops);

-- Query MUST use <=> for cosine
SELECT * FROM document_chunks
ORDER BY embedding <=> '[...]'::vector  -- ‚úÖ Matches index
LIMIT 10;

-- For L2 distance:
CREATE INDEX chunks_embedding_l2_idx
ON document_chunks
USING hnsw (embedding vector_l2_ops);

-- Query MUST use <-> for L2
SELECT * FROM document_chunks
ORDER BY embedding <-> '[...]'::vector  -- ‚úÖ Matches index
LIMIT 10;
```

**HNSW vs IVFFlat Tuning**:

```sql
-- HNSW: Better recall, slower build, more memory
CREATE INDEX chunks_hnsw_idx
ON document_chunks
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);  -- Defaults

-- Tune for better recall (slower queries):
SET hnsw.ef_search = 100;  -- Default: 40

-- IVFFlat: Faster build, less memory, worse recall
-- Lists = sqrt(rows) to rows/1000
CREATE INDEX chunks_ivfflat_idx
ON document_chunks
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);  -- For ~100K rows

-- Tune for better recall (slower queries):
SET ivfflat.probes = 10;  -- Default: 1
```

**Performance Comparison** (from documentation):

| Scenario | No Index | IVFFlat | HNSW |
|----------|----------|---------|------|
| 100K rows, exact search | 850ms | 45ms | 12ms |
| Build time (100K rows) | N/A | 8 seconds | 45 seconds |
| Memory usage | Low | Medium | High |
| Recall @ 10 | 100% | 95% | 98% |

**Additional Resources**:
- pgvector Performance Tips: https://www.crunchydata.com/blog/pgvector-performance-for-developers
- HNSW Index Guide: https://www.crunchydata.com/blog/hnsw-indexes-with-postgres-and-pgvector

---

### 7. OpenAI Rate Limiting (Request Throttling)

**Severity**: High
**Category**: API Integration / Performance
**Affects**: OpenAI Embeddings API
**Source**: https://platform.openai.com/docs/guides/rate-limits

**What it is**:
OpenAI imposes rate limits on API requests: 3,000 requests/minute (RPM) and 1,000,000 tokens/minute (TPM) for tier 1 accounts. Exceeding these causes 429 errors and exponential backoff delays.

**Why it's a problem**:
- Document ingestion takes 10x longer with rate limit retries
- User experience degrades (slow uploads)
- Unpredictable - depends on account tier and current usage
- Easy to hit with batch operations

**How to detect it**:
- Error: `openai.RateLimitError: Rate limit reached for requests`
- Header: `x-ratelimit-remaining-requests: 0`
- Symptom: Ingestion slows down progressively, then fails

**How to avoid/fix**:

```python
# ‚ùå WRONG - No rate limiting, no batching
async def embed_documents(texts: list[str]) -> list[list[float]]:
    embeddings = []
    for text in texts:  # 1000 texts = 1000 API calls
        response = await openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=text  # üíÄ One request per text!
        )
        embeddings.append(response.data[0].embedding)
    return embeddings
# Result: Hits rate limit after 3000 texts, takes 10+ minutes

# ‚úÖ RIGHT - Batch requests + exponential backoff
import asyncio
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)

@retry(
    retry=retry_if_exception_type(openai.RateLimitError),
    wait=wait_exponential(multiplier=1, min=4, max=60),
    stop=stop_after_attempt(5)
)
async def embed_batch_with_retry(
    texts: list[str],
    model: str = "text-embedding-3-small"
) -> list[list[float]]:
    """Embed batch with automatic retry on rate limits."""
    response = await openai_client.embeddings.create(
        model=model,
        input=texts  # Batch up to 100 texts
    )
    return [item.embedding for item in response.data]

async def embed_documents_efficiently(
    texts: list[str],
    batch_size: int = 100  # OpenAI allows up to 100
) -> list[list[float]]:
    """Embed with batching and rate limiting."""
    all_embeddings = []

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]

        try:
            embeddings = await embed_batch_with_retry(batch)
            all_embeddings.extend(embeddings)

            # Rate limiting: 3000 RPM = 50 RPS
            # With batch_size=100, that's 5000 texts/sec
            # Add small delay to stay under limit
            await asyncio.sleep(0.05)  # 20 batches/sec = 2000 texts/sec

        except openai.RateLimitError as e:
            logger.error(f"Rate limit hit after {len(all_embeddings)} embeddings")
            # Save progress
            await save_partial_embeddings(texts[:len(all_embeddings)], all_embeddings)
            raise

    return all_embeddings

# Why this works:
# 1. Batching reduces API calls 100x (1000 texts = 10 calls, not 1000)
# 2. Exponential backoff handles transient rate limits gracefully
# 3. Small delays prevent hitting limit in first place
# 4. Saves progress on failure (can resume)

# Advanced: Parallel batches with semaphore
async def embed_documents_parallel(
    texts: list[str],
    batch_size: int = 100,
    max_concurrent: int = 10  # Limit concurrent requests
) -> list[list[float]]:
    """Embed with parallel batches and rate limiting."""
    semaphore = asyncio.Semaphore(max_concurrent)

    async def embed_batch_limited(batch: list[str]) -> list[list[float]]:
        async with semaphore:  # Limit concurrency
            return await embed_batch_with_retry(batch)

    # Create tasks for all batches
    tasks = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        tasks.append(embed_batch_limited(batch))

    # Execute in parallel (up to max_concurrent at once)
    results = await asyncio.gather(*tasks)

    # Flatten results
    all_embeddings = []
    for batch_embeddings in results:
        all_embeddings.extend(batch_embeddings)

    return all_embeddings

# Result: 1000 texts in ~10 seconds (vs 5+ minutes without batching)
```

**Rate Limit Tiers** (as of 2025):

| Tier | RPM | TPM | Batch Queue Limit |
|------|-----|-----|-------------------|
| Free | 500 | 150,000 | - |
| Tier 1 | 3,000 | 1,000,000 | 100,000 |
| Tier 2 | 3,500 | 5,000,000 | 500,000 |
| Tier 3+ | Higher | Higher | Higher |

**Additional Resources**:
- OpenAI Rate Limit Guide: https://platform.openai.com/docs/guides/rate-limits
- Handling Rate Limits Cookbook: https://cookbook.openai.com/examples/how_to_handle_rate_limits

---

### 8. Document Chunking Context Loss

**Severity**: High
**Category**: Search Quality / Data Loss
**Affects**: Document ingestion pipeline
**Source**: https://www.f22labs.com/blogs/7-chunking-strategies-in-rag-you-need-to-know/

**What it is**:
Fixed-size chunking (e.g., splitting every 500 characters) splits sentences, paragraphs, and logical units mid-thought, causing context loss. Search retrieves incomplete or confusing chunks that hurt LLM response quality.

**Why it's a problem**:
- Retrieved chunks lack critical context
- LLM generates incorrect or incomplete answers
- Users get frustrated with poor search results
- No way to recover lost context after ingestion

**How to detect it**:
- Symptom: Search returns chunks that start/end mid-sentence
- Symptom: LLM asks "I need more context" or gives generic answers
- Test: Manually review top 10 search results for common queries

**How to avoid/fix**:

```python
# ‚ùå WRONG - Fixed-size chunking (destroys context)
def chunk_document_badly(text: str, chunk_size: int = 500) -> list[str]:
    chunks = []
    for i in range(0, len(text), chunk_size):
        chunk = text[i:i+chunk_size]  # üíÄ Splits mid-sentence
        chunks.append(chunk)
    return chunks

# Example result:
# Chunk 1: "...neural networks are powerful. They can lear"
# Chunk 2: "n complex patterns from data. Deep learning mod"
# Chunk 3: "els use multiple layers. The activation function"
#
# Problem: Each chunk is incomplete and confusing!

# ‚úÖ RIGHT - Semantic chunking with overlap
from typing import List, Tuple

def chunk_document_semantic(
    text: str,
    max_chunk_size: int = 1000,
    overlap: int = 200,
    split_on: tuple[str, ...] = ("\n\n", "\n", ". ", "! ", "? ")
) -> list[dict]:
    """
    Chunk text at semantic boundaries with overlap.

    Args:
        text: Full document text
        max_chunk_size: Maximum chunk size in characters
        overlap: Overlap between chunks (for context continuity)
        split_on: Delimiters to split on (in priority order)

    Returns:
        List of chunk dicts with text and metadata
    """
    chunks = []
    start = 0

    while start < len(text):
        # Determine chunk end
        end = start + max_chunk_size

        # If this is the last chunk, take everything
        if end >= len(text):
            chunk_text = text[start:]
            chunks.append({
                "text": chunk_text.strip(),
                "start_char": start,
                "end_char": len(text),
                "chunk_index": len(chunks)
            })
            break

        # Try to split at semantic boundary
        split_point = None
        for delimiter in split_on:
            # Look for delimiter near the end of chunk
            search_start = max(start + max_chunk_size - 100, start)
            search_end = min(start + max_chunk_size, len(text))

            # Find last occurrence of delimiter in search window
            pos = text.rfind(delimiter, search_start, search_end)
            if pos != -1:
                split_point = pos + len(delimiter)
                break

        # Fallback: Split at max_chunk_size if no delimiter found
        if split_point is None:
            split_point = end

        # Extract chunk
        chunk_text = text[start:split_point]
        chunks.append({
            "text": chunk_text.strip(),
            "start_char": start,
            "end_char": split_point,
            "chunk_index": len(chunks)
        })

        # Move start forward, accounting for overlap
        start = split_point - overlap

        # Ensure we make progress (don't get stuck)
        if start <= chunks[-1]["start_char"]:
            start = split_point

    return chunks

# Why this works:
# 1. Splits at paragraph/sentence boundaries (preserves meaning)
# 2. Overlap ensures context continuity between chunks
# 3. Falls back to hard split only when no delimiter found
# 4. Tracks positions for debugging

# Advanced: Add contextual metadata
def chunk_with_metadata(
    document_id: str,
    title: str,
    text: str,
    max_chunk_size: int = 1000
) -> list[dict]:
    """Chunk with added context metadata."""
    base_chunks = chunk_document_semantic(text, max_chunk_size)

    enriched_chunks = []
    for chunk in base_chunks:
        # Add document context to each chunk
        enriched_text = f"Document: {title}\n\n{chunk['text']}"

        # Add surrounding context for better retrieval
        prev_chunk = enriched_chunks[-1]['text'] if enriched_chunks else ""
        prev_preview = prev_chunk[-100:] if prev_chunk else ""

        enriched_chunks.append({
            "document_id": document_id,
            "chunk_index": chunk["chunk_index"],
            "text": chunk["text"],
            "text_with_context": enriched_text,
            "previous_chunk_preview": prev_preview,
            "char_count": len(chunk["text"]),
            "word_count": len(chunk["text"].split())
        })

    return enriched_chunks

# Result: Coherent chunks that preserve meaning
# Chunk 1: "Neural networks are powerful machine learning models.
#           They can learn complex patterns from data by adjusting weights."
# Chunk 2 (with overlap): "...by adjusting weights. Deep learning models
#           use multiple layers of neurons. The activation function determines
#           whether a neuron fires."
```

**Chunking Strategy Comparison**:

| Strategy | Pros | Cons | Best For |
|----------|------|------|----------|
| Fixed-size | Fast, simple | Breaks context | Never recommended |
| Sentence-based | Preserves sentences | Variable size | Short documents |
| Paragraph-based | Preserves ideas | Very variable size | Blog posts |
| Semantic (recommended) | Best context | Complex | All documents |
| Recursive | Handles structure | Slow | Markdown, code |

**Additional Resources**:
- Chunking Guide: https://www.f22labs.com/blogs/7-chunking-strategies-in-rag-you-need-to-know/
- LangChain Text Splitters: https://python.langchain.com/docs/modules/data_connection/document_transformers/

---

### 9. pgvector Dimension Limit Exceeded

**Severity**: High
**Category**: Configuration / Storage
**Affects**: PostgreSQL pgvector extension
**Source**: https://github.com/pgvector/pgvector

**What it is**:
pgvector can only index vectors with up to 2000 dimensions. Attempting to create an index on a vector column with >2000 dimensions fails with an error. Also, vector columns must have dimensions explicitly defined (`vector(1536)`, not just `vector`).

**Why it's a problem**:
- Can't use large embedding models (e.g., OpenAI text-embedding-3-large with 3072 dimensions)
- Error doesn't appear until you try to create index (not at column creation)
- No index = slow queries (full table scans)
- Forces you to use smaller models or switch to dedicated vector DB

**How to detect it**:
- Error: `ERROR: column vector dimension must be less than or equal to 2000`
- Error: `ERROR: column must have dimensions to create an index`
- Symptom: Index creation fails but column creation succeeds

**How to avoid/fix**:

```sql
-- ‚ùå WRONG - No dimension specified, or dimension > 2000
CREATE TABLE document_chunks (
    id SERIAL PRIMARY KEY,
    text TEXT,
    embedding vector  -- üíÄ No dimension specified!
);

-- This fails!
CREATE INDEX embedding_idx ON document_chunks
USING hnsw (embedding vector_cosine_ops);
-- ERROR: column must have dimensions to create an index

-- Also wrong - dimension too large
CREATE TABLE document_chunks (
    id SERIAL PRIMARY KEY,
    text TEXT,
    embedding vector(3072)  -- OpenAI text-embedding-3-large
);

-- This also fails!
CREATE INDEX embedding_idx ON document_chunks
USING hnsw (embedding vector_cosine_ops);
-- ERROR: column vector dimension must be less than or equal to 2000

-- ‚úÖ RIGHT - Specify dimensions ‚â§ 2000
CREATE TABLE document_chunks (
    id SERIAL PRIMARY KEY,
    document_id UUID NOT NULL,
    chunk_index INTEGER NOT NULL,
    text TEXT NOT NULL,
    embedding vector(1536) NOT NULL,  -- OpenAI text-embedding-3-small
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Index creation succeeds
CREATE INDEX embedding_cosine_idx ON document_chunks
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- Verify index is created
\d document_chunks
-- Indexes:
--   "embedding_cosine_idx" hnsw (embedding vector_cosine_ops)
```

**Workaround for Large Models** (>2000 dimensions):

```python
# Option 1: Use OpenAI dimensions parameter (text-embedding-3-large only)
# Reduce 3072 -> 1536 dimensions with minimal quality loss
response = await openai_client.embeddings.create(
    model="text-embedding-3-large",
    input=texts,
    dimensions=1536  # Reduce from native 3072
)

# Option 2: Use dimensionality reduction (PCA)
from sklearn.decomposition import PCA
import numpy as np

def reduce_dimensions(embeddings: list[list[float]], target_dim: int = 1536) -> list[list[float]]:
    """Reduce embedding dimensions using PCA."""
    # Fit PCA on your corpus (do this once during setup)
    pca = PCA(n_components=target_dim)
    embeddings_array = np.array(embeddings)
    reduced = pca.fit_transform(embeddings_array)
    return reduced.tolist()

# Option 3: Switch to Qdrant or Weaviate (no dimension limit)
# Qdrant supports arbitrary dimensions efficiently
from qdrant_client import AsyncQdrantClient
from qdrant_client.models import Distance, VectorParams

client = AsyncQdrantClient("localhost")
await client.create_collection(
    collection_name="documents",
    vectors_config=VectorParams(
        size=3072,  # ‚úÖ No problem with Qdrant
        distance=Distance.COSINE
    )
)
```

**Dimension Recommendations**:

| Model | Native Dim | Indexed in pgvector? | Recommendation |
|-------|-----------|---------------------|----------------|
| text-embedding-3-small | 1536 | ‚úÖ Yes | Use directly |
| text-embedding-3-large | 3072 | ‚ùå No (too large) | Reduce to 1536 or use Qdrant |
| all-MiniLM-L6-v2 | 384 | ‚úÖ Yes | Use directly |
| all-mpnet-base-v2 | 768 | ‚úÖ Yes | Use directly |

**Additional Resources**:
- pgvector GitHub Issues: https://github.com/pgvector/pgvector/issues

---

### 10. Qdrant Memory Exhaustion (Index Growth)

**Severity**: High
**Category**: Resource Management / Performance
**Affects**: Qdrant HNSW index
**Source**: https://dagshub.com/blog/common-pitfalls-to-avoid-when-using-vector-databases/

**What it is**:
Qdrant's HNSW index stores the entire vector graph in RAM for fast queries. As your collection grows beyond available memory, the index spills to disk, causing dramatic performance degradation (10-100x slower queries).

**Why it's a problem**:
- Queries go from 10ms to 1000ms+ when RAM exhausted
- Unpredictable - depends on server RAM and collection size
- Disk I/O becomes bottleneck
- Requires expensive RAM upgrades or collection sharding

**How to detect it**:
- Symptom: Queries slow down progressively as data grows
- Monitor: Qdrant memory usage approaching server RAM limit
- Docker: Container OOMKilled or slow queries after 100K+ vectors
- Test: Query latency P99 >100ms with collection in disk cache

**How to avoid/fix**:

```python
# ‚ùå WRONG - No memory consideration, high-dim vectors
from qdrant_client import AsyncQdrantClient
from qdrant_client.models import Distance, VectorParams

# Store 1M vectors @ 3072 dimensions = ~12GB RAM minimum
# (1M * 3072 * 4 bytes/float = 12GB, + HNSW overhead ~2x = 24GB)
client = AsyncQdrantClient("localhost")
await client.create_collection(
    collection_name="documents",
    vectors_config=VectorParams(
        size=3072,  # üíÄ High dimensions = more memory
        distance=Distance.COSINE
    )
)
# Result: Runs out of RAM at ~500K vectors on 16GB server

# ‚úÖ RIGHT - Memory-efficient configuration
# Option 1: Use lower-dimensional embeddings
# 1M vectors @ 384 dimensions = ~1.5GB RAM minimum
await client.create_collection(
    collection_name="documents",
    vectors_config=VectorParams(
        size=384,  # all-MiniLM-L6-v2 embeddings
        distance=Distance.COSINE
    )
)

# Option 2: Enable quantization (4x memory reduction)
from qdrant_client.models import ScalarQuantization, ScalarType, QuantizationConfig

await client.create_collection(
    collection_name="documents",
    vectors_config=VectorParams(
        size=1536,  # OpenAI text-embedding-3-small
        distance=Distance.COSINE
    ),
    quantization_config=ScalarQuantization(
        scalar=ScalarType.INT8  # 4 bytes -> 1 byte per dimension
    )
)
# Result: 1M vectors @ 1536 dims = ~1.5GB instead of ~6GB

# Option 3: Use on-disk storage (sacrifices speed for capacity)
from qdrant_client.models import OptimizersConfigDiff

await client.update_collection(
    collection_name="documents",
    optimizer_config=OptimizersConfigDiff(
        memmap_threshold=20000  # Keep only 20K vectors in RAM
    )
)
# Result: Can store 10M+ vectors, but queries 10x slower

# Why this works:
# 1. Lower dimensions = less memory per vector
# 2. Quantization reduces memory 4x with minimal quality loss
# 3. Disk storage trades speed for capacity
```

**Memory Estimation Formula**:

```python
def estimate_qdrant_memory(
    num_vectors: int,
    dimension: int,
    quantized: bool = False
) -> dict:
    """Estimate Qdrant memory usage."""
    bytes_per_float = 4 if not quantized else 1

    # Vector data
    vector_data = num_vectors * dimension * bytes_per_float

    # HNSW index overhead (~2x vector data)
    hnsw_overhead = vector_data * 2

    total_bytes = vector_data + hnsw_overhead
    total_gb = total_bytes / (1024**3)

    return {
        "vectors": num_vectors,
        "dimension": dimension,
        "quantized": quantized,
        "vector_data_gb": vector_data / (1024**3),
        "hnsw_overhead_gb": hnsw_overhead / (1024**3),
        "total_memory_gb": total_gb,
        "recommended_ram_gb": total_gb * 1.5  # 50% headroom
    }

# Examples:
# 100K vectors, 1536 dim, no quantization
# = 100K * 1536 * 4 * 3 = 1.8GB (need 3GB RAM)

# 1M vectors, 1536 dim, no quantization
# = 1M * 1536 * 4 * 3 = 18GB (need 27GB RAM)

# 1M vectors, 1536 dim, quantized
# = 1M * 1536 * 1 * 3 = 4.5GB (need 7GB RAM)
```

**Scaling Strategy**:

```yaml
# docker-compose.yml - Set memory limits
services:
  qdrant:
    image: qdrant/qdrant:latest
    mem_limit: 8g  # Explicit memory limit
    environment:
      - QDRANT__SERVICE__MAX_REQUEST_SIZE_MB=32
    volumes:
      - ./qdrant_storage:/qdrant/storage:z
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
```

**When to Shard Collections**:

```python
# If you need >10M vectors or >100GB RAM, shard by source/date
async def create_sharded_collections(sources: list[str]):
    """Create separate collection per source."""
    for source in sources:
        await client.create_collection(
            collection_name=f"documents_{source}",
            vectors_config=VectorParams(size=1536, distance=Distance.COSINE),
            quantization_config=ScalarQuantization(scalar=ScalarType.INT8)
        )

# Query across shards
async def search_all_shards(query_vector: list[float], sources: list[str]):
    tasks = [
        client.search(
            collection_name=f"documents_{source}",
            query_vector=query_vector,
            limit=5
        )
        for source in sources
    ]
    results = await asyncio.gather(*tasks)
    # Merge and re-rank
    return merge_results(results)
```

**Additional Resources**:
- Qdrant Configuration Guide: https://qdrant.tech/documentation/guides/configuration/
- Quantization Documentation: https://qdrant.tech/documentation/quantization/

---

## Medium Priority Gotchas

### 11. MCP Tool Response Size Limit

**Severity**: Medium
**Category**: API Quirk / Protocol Limitation
**Affects**: MCP tools returning large content
**Source**: Task-manager gotcha #3 from codebase-patterns.md

**What it is**:
MCP (Model Context Protocol) tools have practical size limits for responses. Returning full document content (50KB+) through MCP tools exceeds token limits for AI assistants and causes poor UX. The protocol doesn't enforce limits, but AI assistants struggle with large responses.

**Why it's confusing**:
- No explicit error - just slow/poor AI responses
- Developers expect to return full content like REST APIs
- MCP protocol documentation doesn't specify size limits
- Works fine in testing (small docs), fails in production (large docs)

**How to handle it**:

```python
# ‚ùå WRONG - Return full document content via MCP
@mcp.tool()
async def search_documents(query: str, limit: int = 10) -> str:
    """Search documents and return full content."""
    results = await search_service.search(query, limit)

    # Each document might be 50KB of text
    # 10 documents = 500KB response = üíÄ Too much for AI assistant
    return json.dumps({
        "success": True,
        "documents": [
            {
                "id": doc.id,
                "title": doc.title,
                "content": doc.content,  # Full 50KB content!
                "metadata": doc.metadata
            }
            for doc in results
        ]
    })

# ‚úÖ RIGHT - Truncate large fields for MCP
MAX_CONTENT_LENGTH = 1000  # characters
MAX_PER_PAGE = 20  # items

def truncate_text(text: str | None, max_length: int = MAX_CONTENT_LENGTH) -> str | None:
    """Truncate text with ellipsis."""
    if text and len(text) > max_length:
        return text[:max_length - 3] + "..."
    return text

def optimize_for_mcp(document: dict) -> dict:
    """Optimize document for MCP response."""
    doc = document.copy()

    # Truncate large text fields
    if "content" in doc:
        doc["content"] = truncate_text(doc["content"])
    if "description" in doc:
        doc["description"] = truncate_text(doc["description"], 500)

    # Remove unnecessary fields
    doc.pop("raw_html", None)
    doc.pop("full_metadata", None)

    return doc

@mcp.tool()
async def search_documents(
    query: str,
    limit: int = 10,
    include_full_content: bool = False
) -> str:
    """
    Search documents with optimized MCP responses.

    Args:
        query: Search query text
        limit: Number of results (max 20 for MCP)
        include_full_content: If False, content is truncated to 1000 chars

    Returns:
        JSON string with search results
    """
    # Enforce max limit for MCP
    limit = min(limit, MAX_PER_PAGE)

    results = await search_service.search(query, limit)

    # Optimize each document
    if not include_full_content:
        documents = [optimize_for_mcp(doc) for doc in results]
    else:
        documents = results

    # CRITICAL: MCP tools MUST return JSON strings, never dicts
    return json.dumps({
        "success": True,
        "documents": documents,
        "total_found": len(results),
        "truncated": not include_full_content,
        "hint": "Use get_document(id) to retrieve full content"
    })

@mcp.tool()
async def get_document(document_id: str) -> str:
    """Get single document with full content."""
    doc = await document_service.get_document(document_id)

    if not doc:
        return json.dumps({
            "success": False,
            "error": "Document not found",
            "suggestion": f"Verify document ID {document_id} exists"
        })

    # For single document, full content is OK
    return json.dumps({
        "success": True,
        "document": doc
    })

# Why this works:
# 1. List operations return truncated content (small payload)
# 2. Single get operations return full content (acceptable size)
# 3. AI assistant gets enough context from truncated content
# 4. Can fetch full content if needed with explicit get_document() call
# 5. Limits pagination to prevent huge responses
```

**Response Size Guidelines**:

| Operation | Max Items | Content per Item | Total Size |
|-----------|-----------|------------------|------------|
| Search/List | 20 | 1000 chars | ~20KB |
| Get Single | 1 | Full content | ~50KB |
| Bulk Get | 5 | Full content | ~250KB (stretch limit) |

**Conditional Field Selection Pattern**:

```python
# Service layer supports field exclusion
class DocumentService:
    async def list_documents(
        self,
        limit: int = 50,
        exclude_large_fields: bool = False
    ) -> tuple[bool, dict]:
        """List documents with optional field exclusion."""
        async with self.db_pool.acquire() as conn:
            if exclude_large_fields:
                # Truncate large fields in SQL
                query = """
                    SELECT
                        id, title, source_id, created_at,
                        CASE
                            WHEN LENGTH(content) > 1000
                            THEN SUBSTRING(content FROM 1 FOR 1000) || '...'
                            ELSE content
                        END as content
                    FROM documents
                    LIMIT $1
                """
            else:
                query = "SELECT * FROM documents LIMIT $1"

            rows = await conn.fetch(query, limit)

        return True, {"documents": [dict(row) for row in rows]}

# MCP tool uses exclude_large_fields=True
@mcp.tool()
async def find_documents() -> str:
    success, result = await document_service.list_documents(
        limit=20,
        exclude_large_fields=True  # MCP optimization
    )
    return json.dumps(result)
```

**Additional Resources**:
- Task-manager MCP patterns: `/infra/task-manager/backend/src/mcp_server.py`

---

### 12. PostgreSQL ts_vector Not Updated Automatically

**Severity**: Medium
**Category**: Search Quality / Configuration
**Affects**: PostgreSQL full-text search (ts_vector)
**Source**: https://www.postgresql.org/docs/current/textsearch-tables.html

**What it is**:
`tsvector` columns don't automatically update when the source text columns change unless you set up a trigger. Without triggers, your full-text search index becomes stale, returning outdated results.

**Why it's confusing**:
- Inserting/updating documents succeeds without errors
- Search works initially (if you manually set tsvector)
- Results become stale over time as documents change
- No error message - just wrong results

**How to handle**:

```sql
-- ‚ùå WRONG - Manual tsvector updates (easy to forget)
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    title TEXT,
    content TEXT,
    search_vector tsvector
);

-- Developer must remember to update search_vector every time
INSERT INTO documents (title, content, search_vector)
VALUES (
    'My Document',
    'This is the content',
    to_tsvector('english', 'My Document This is the content')  -- üíÄ Manual!
);

-- Updating title/content doesn't update search_vector!
UPDATE documents
SET content = 'Updated content'
WHERE id = 1;
-- üíÄ search_vector still has old content!

-- ‚úÖ RIGHT - Automatic trigger updates
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    title TEXT NOT NULL,
    content TEXT NOT NULL,
    search_vector tsvector GENERATED ALWAYS AS (
        to_tsvector('english', coalesce(title, '') || ' ' || coalesce(content, ''))
    ) STORED  -- PostgreSQL 12+ generated column
);

-- Alternative: Trigger for older PostgreSQL or complex logic
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    title TEXT NOT NULL,
    content TEXT NOT NULL,
    search_vector tsvector
);

-- Trigger function
CREATE OR REPLACE FUNCTION documents_search_vector_update()
RETURNS TRIGGER AS $$
BEGIN
    NEW.search_vector :=
        setweight(to_tsvector('english', coalesce(NEW.title, '')), 'A') ||
        setweight(to_tsvector('english', coalesce(NEW.content, '')), 'B');
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Attach trigger
CREATE TRIGGER documents_search_vector_trigger
BEFORE INSERT OR UPDATE ON documents
FOR EACH ROW
EXECUTE FUNCTION documents_search_vector_update();

-- Now updates are automatic!
INSERT INTO documents (title, content)
VALUES ('My Document', 'This is the content');
-- search_vector automatically populated

UPDATE documents
SET content = 'Updated content'
WHERE id = 1;
-- search_vector automatically updated ‚úÖ

-- Create GIN index for fast search
CREATE INDEX documents_search_idx
ON documents
USING GIN (search_vector);

-- Search with ranking
SELECT
    id,
    title,
    ts_rank(search_vector, query) AS rank
FROM documents,
     to_tsquery('english', 'content & update') query
WHERE search_vector @@ query
ORDER BY rank DESC
LIMIT 10;
```

**Weighted Search** (title more important than content):

```sql
-- Assign weights: A = title (weight 1.0), B = content (weight 0.4)
CREATE OR REPLACE FUNCTION documents_search_vector_update()
RETURNS TRIGGER AS $$
BEGIN
    NEW.search_vector :=
        setweight(to_tsvector('english', coalesce(NEW.title, '')), 'A') ||
        setweight(to_tsvector('english', coalesce(NEW.content, '')), 'B');
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Search with weighted ranking
SELECT
    id,
    title,
    ts_rank('{0.1, 0.2, 0.4, 1.0}', search_vector, query) AS rank
FROM documents,
     to_tsquery('english', 'postgresql & search') query
WHERE search_vector @@ query
ORDER BY rank DESC;
```

**Bulk Insert Optimization**:

```sql
-- For large initial loads, disable trigger temporarily
ALTER TABLE documents DISABLE TRIGGER documents_search_vector_trigger;

-- Bulk insert
COPY documents (title, content) FROM '/data/documents.csv' CSV;

-- Update tsvector in batch (faster than per-row trigger)
UPDATE documents
SET search_vector =
    setweight(to_tsvector('english', coalesce(title, '')), 'A') ||
    setweight(to_tsvector('english', coalesce(content, '')), 'B')
WHERE search_vector IS NULL;

-- Re-enable trigger
ALTER TABLE documents ENABLE TRIGGER documents_search_vector_trigger;

-- Rebuild index
REINDEX INDEX documents_search_idx;
```

**Additional Resources**:
- PostgreSQL Text Search Documentation: https://www.postgresql.org/docs/current/textsearch-tables.html

---

### 13. crawl4ai Browser Binary Not Installed

**Severity**: Medium
**Category**: Setup / Dependencies
**Affects**: crawl4ai web crawler
**Source**: https://docs.crawl4ai.com/core/installation/

**What it is**:
crawl4ai requires Playwright browser binaries (Chromium) to be installed separately via `crawl4ai-setup`. Just `pip install crawl4ai` is insufficient - the first crawl attempt fails with "Browser not found".

**Why it's confusing**:
- `pip install` succeeds without warning
- Error only appears when you try to crawl (not at import time)
- Error message is cryptic: `Playwright... chromium-...not found`
- Not mentioned prominently in quickstart docs

**How to handle**:

```bash
# ‚ùå WRONG - Only pip install
pip install crawl4ai

# Try to use it:
python -c "from crawl4ai import AsyncWebCrawler; import asyncio; asyncio.run(AsyncWebCrawler().arun('https://example.com'))"
# Error: playwright._impl._api_types.Error: Executable doesn't exist at /home/user/.cache/ms-playwright/chromium-1091/chrome-linux/chrome

# ‚úÖ RIGHT - Install package + browser binaries
pip install crawl4ai
crawl4ai-setup  # Downloads Chromium (~100MB)

# Or use playwright directly:
pip install crawl4ai
playwright install chromium

# Now works:
python -c "from crawl4ai import AsyncWebCrawler; import asyncio; asyncio.run(AsyncWebCrawler().arun('https://example.com'))"
```

**Docker Setup** (for production):

```dockerfile
# Dockerfile for RAG service with crawl4ai
FROM python:3.11-slim

# Install system dependencies for Playwright
RUN apt-get update && apt-get install -y \
    wget \
    gnupg \
    ca-certificates \
    fonts-liberation \
    libasound2 \
    libatk-bridge2.0-0 \
    libatk1.0-0 \
    libatspi2.0-0 \
    libcups2 \
    libdbus-1-3 \
    libdrm2 \
    libgbm1 \
    libgtk-3-0 \
    libnspr4 \
    libnss3 \
    libwayland-client0 \
    libxcomposite1 \
    libxdamage1 \
    libxfixes3 \
    libxkbcommon0 \
    libxrandr2 \
    xdg-utils \
    && rm -rf /var/lib/apt/lists/*

# Install Python packages
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Install browser binaries
RUN crawl4ai-setup

WORKDIR /app
COPY . .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Validation Test**:

```python
# test_crawl4ai_setup.py
import asyncio
from crawl4ai import AsyncWebCrawler

async def test_browser_installed():
    """Verify browser binaries are installed."""
    try:
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun("https://example.com")
            assert result.success, "Crawl failed"
            assert len(result.markdown) > 0, "No content retrieved"
            print("‚úÖ crawl4ai setup successful")
            return True
    except Exception as e:
        if "Executable doesn't exist" in str(e):
            print("‚ùå Browser binaries not installed. Run: crawl4ai-setup")
        else:
            print(f"‚ùå Unexpected error: {e}")
        return False

if __name__ == "__main__":
    asyncio.run(test_browser_installed())
```

**Additional Resources**:
- crawl4ai Installation Guide: https://docs.crawl4ai.com/core/installation/

---

### 14. Qdrant Distance Metric Mismatch

**Severity**: Medium
**Category**: API Quirk / Search Quality
**Affects**: Qdrant vector search
**Source**: https://qdrant.tech/documentation/

**What it is**:
Qdrant collections are created with a specific distance metric (Cosine, Euclidean, Dot Product). If you query with a different search approach or misunderstand the metric, search results are wrong or poorly ranked.

**Why it's confusing**:
- No error - queries complete successfully
- Results just seem "off" or poorly ranked
- Cosine distance is 0-2, but intuition expects 0-1 similarity
- Dot product requires normalized vectors (easy to forget)

**How to handle**:

```python
# ‚ùå WRONG - Mismatched distance expectations
from qdrant_client import AsyncQdrantClient
from qdrant_client.models import Distance, VectorParams

# Create collection with COSINE distance
client = AsyncQdrantClient("localhost")
await client.create_collection(
    collection_name="documents",
    vectors_config=VectorParams(
        size=1536,
        distance=Distance.COSINE  # Expects normalized vectors
    )
)

# Insert non-normalized vectors
import numpy as np
vector = [0.5, 0.3, 0.8, ...]  # 1536 dimensions
# üíÄ Not normalized! Magnitude ‚â† 1

await client.upsert(
    collection_name="documents",
    points=[{"id": 1, "vector": vector}]
)

# Results are suboptimal because vectors aren't normalized

# ‚úÖ RIGHT - Normalize vectors for Cosine distance
def normalize_vector(vector: list[float]) -> list[float]:
    """Normalize vector to unit length for Cosine distance."""
    magnitude = np.linalg.norm(vector)
    if magnitude == 0:
        return vector  # Avoid division by zero
    return (np.array(vector) / magnitude).tolist()

# OpenAI embeddings are already normalized, but verify:
response = await openai_client.embeddings.create(
    model="text-embedding-3-small",
    input="test text"
)
vector = response.data[0].embedding

# Verify normalization (magnitude should be ~1.0)
magnitude = np.linalg.norm(vector)
print(f"Vector magnitude: {magnitude}")  # ~1.0 for OpenAI

# For custom embeddings, always normalize:
custom_vector = [0.5, 0.3, 0.8, ...]  # Your embedding
normalized_vector = normalize_vector(custom_vector)

await client.upsert(
    collection_name="documents",
    points=[{"id": 1, "vector": normalized_vector}]
)

# Search also with normalized query
query_vector = normalize_vector(query_embedding)
results = await client.query_points(
    collection_name="documents",
    query=query_vector,
    limit=10
)

# Why this works:
# 1. Cosine distance assumes normalized vectors
# 2. Non-normalized vectors give incorrect distances
# 3. OpenAI embeddings are pre-normalized
# 4. Custom embeddings must be normalized manually
```

**Distance Metric Comparison**:

```python
# Choose distance metric based on embedding type
from qdrant_client.models import Distance

# Cosine (recommended for most text embeddings)
# - Range: 0-2 (0 = identical, 2 = opposite)
# - Converts to similarity: similarity = 1 - (distance / 2)
# - Requires normalized vectors
# - Invariant to vector magnitude
vectors_config=VectorParams(size=1536, distance=Distance.COSINE)

# Euclidean (L2)
# - Range: 0-‚àû (0 = identical)
# - Good for embeddings with meaningful magnitudes
# - Does NOT require normalization
vectors_config=VectorParams(size=1536, distance=Distance.EUCLID)

# Dot Product
# - Range: -‚àû to ‚àû (higher = more similar)
# - Requires normalized vectors (else unbounded)
# - Fastest computation
# - Equivalent to Cosine for normalized vectors
vectors_config=VectorParams(size=1536, distance=Distance.DOT)

# Example: Convert Cosine distance to similarity score
def cosine_distance_to_similarity(distance: float) -> float:
    """Convert Qdrant Cosine distance to 0-1 similarity."""
    # Distance range: 0-2
    # Similarity range: 0-1
    return 1 - (distance / 2)

# Search with similarity scores
results = await client.query_points(
    collection_name="documents",
    query=query_vector,
    limit=10
)

for point in results:
    distance = point.score  # Qdrant calls it "score" but it's distance
    similarity = cosine_distance_to_similarity(distance)
    print(f"Document {point.id}: similarity={similarity:.3f} (distance={distance:.3f})")
```

**Additional Resources**:
- Qdrant Distance Metrics: https://qdrant.tech/documentation/concepts/search/

---

## Low Priority Gotchas

### 15. Docling Memory Usage on Large PDFs

**Severity**: Low
**Category**: Resource Management
**Affects**: Docling document parser
**Source**: https://docling-project.github.io/docling/

**What it is**:
Docling loads entire PDFs into memory for parsing. Large PDFs (100+ pages, embedded images) can consume 1GB+ RAM per document, causing OOM errors or slow processing.

**Why it's a minor issue**:
- Only affects large documents (most docs are <20 pages)
- Easy to work around with chunked processing
- Doesn't affect search quality, just ingestion speed

**How to handle**:

```python
# ‚ùå WRONG - Process all documents in parallel
from docling import DocumentConverter
import asyncio

async def ingest_documents(file_paths: list[str]):
    converter = DocumentConverter()

    # Process 100 PDFs simultaneously = 100GB+ RAM
    tasks = [asyncio.to_thread(converter.convert, path) for path in file_paths]
    results = await asyncio.gather(*tasks)  # üíÄ OOM!

# ‚úÖ RIGHT - Process with concurrency limit
import asyncio
from pathlib import Path

async def ingest_documents_limited(file_paths: list[str], max_concurrent: int = 3):
    """Process documents with memory-aware concurrency limit."""
    converter = DocumentConverter()
    semaphore = asyncio.Semaphore(max_concurrent)

    async def process_one(path: str):
        async with semaphore:
            # Run CPU-bound Docling in thread pool
            result = await asyncio.to_thread(converter.convert, path)

            # Extract and store chunks immediately
            chunks = extract_chunks(result)
            await store_chunks(chunks)

            # Free memory
            del result
            return {"path": path, "chunks": len(chunks)}

    # Process with max 3 concurrent documents
    tasks = [process_one(path) for path in file_paths]
    results = await asyncio.gather(*tasks)

    return results

# Monitor memory usage
import psutil

def check_memory_before_processing(file_path: str, max_memory_pct: float = 80.0):
    """Check if we have enough memory to process document."""
    file_size = Path(file_path).stat().st_size
    estimated_memory = file_size * 10  # Conservative estimate

    memory = psutil.virtual_memory()
    available = memory.available

    if memory.percent > max_memory_pct:
        raise MemoryError(
            f"Memory usage at {memory.percent}%. "
            f"Wait for memory to free up before processing {file_path}"
        )

    if estimated_memory > available:
        raise MemoryError(
            f"Insufficient memory for {file_path}. "
            f"Need ~{estimated_memory / 1024**2:.0f}MB, have {available / 1024**2:.0f}MB"
        )

# Why this works:
# 1. Limits concurrent document processing
# 2. Releases memory immediately after processing
# 3. Checks available memory before starting
```

---

### 16. asyncpg Connection Timeout on Slow Queries

**Severity**: Low
**Category**: Configuration
**Affects**: asyncpg connection pool
**Source**: https://magicstack.github.io/asyncpg/current/api/index.html

**What it is**:
asyncpg's default `command_timeout` is 60 seconds. Slow queries (e.g., complex hybrid search, bulk operations) timeout and fail even though they would eventually succeed.

**Why it's a minor issue**:
- Most queries complete in <100ms
- Only affects complex analytics or bulk operations
- Easy to adjust per-query or globally

**How to handle**:

```python
# ‚ùå WRONG - Uses default 60s timeout for slow operations
async def bulk_reindex_documents(document_ids: list[str]):
    async with pool.acquire() as conn:
        # This might take 2+ minutes for 100K documents
        await conn.execute(
            "UPDATE documents SET search_vector = to_tsvector('english', content) WHERE id = ANY($1)",
            document_ids
        )  # üíÄ Timeout after 60s!

# ‚úÖ RIGHT - Increase timeout for specific slow queries
async def bulk_reindex_documents(document_ids: list[str]):
    async with pool.acquire() as conn:
        # Set longer timeout for this connection
        await conn.execute("SET statement_timeout = '300s'")  # 5 minutes

        # Now safe to run slow query
        result = await conn.execute(
            "UPDATE documents SET search_vector = to_tsvector('english', content) WHERE id = ANY($1)",
            document_ids
        )

        print(f"Reindexed {result} documents")

# Or increase pool-wide default
pool = await asyncpg.create_pool(
    host="localhost",
    database="rag_service",
    command_timeout=120.0,  # 2 minutes instead of 60s
    min_size=10,
    max_size=20
)
```

---

## Library-Specific Quirks

### OpenAI Embeddings API

**Version-Specific Issues**:
- **text-embedding-3-small**: Max 8192 tokens input, 1536 dimensions
- **text-embedding-3-large**: Max 8192 tokens input, 3072 dimensions (can reduce with `dimensions` param)
- **text-embedding-ada-002**: Legacy model, 1536 dimensions (don't use for new projects)

**Common Mistakes**:
1. **Not batching requests**: Send up to 100 texts per request to reduce API calls 100x
2. **Ignoring rate limits**: Tier 1 = 3000 RPM, 1M TPM - easily exceeded without throttling
3. **Not caching embeddings**: Embedding same text multiple times wastes money
4. **Exceeding token limit**: Texts >8192 tokens silently truncated (use tiktoken to count)

**Best Practices**:
- **Batch size**: Use 100 texts per request (max allowed)
- **Rate limiting**: Add 50ms delay between batches to stay under 3000 RPM
- **Retry logic**: Use exponential backoff for 429 errors
- **Cost tracking**: Log token usage per request for cost monitoring

---

### Qdrant

**Version-Specific Issues**:
- **Pre-1.6.0**: No async client support (use sync client with `asyncio.to_thread()`)
- **1.6.0+**: AsyncQdrantClient available, much faster for FastAPI

**Common Mistakes**:
1. **No volume mounting**: Data lost on container restart
2. **Missing API key in production**: Anyone can access your vector DB
3. **Wrong distance metric**: Using Euclidean when embeddings expect Cosine
4. **Not using quantization**: 4x more memory than needed

**Best Practices**:
- **Distance metric**: Use Cosine for text embeddings (OpenAI, Sentence Transformers)
- **Quantization**: Enable INT8 quantization for 4x memory reduction
- **Volume mounting**: Always mount `./qdrant_storage:/qdrant/storage:z`
- **API key**: Set `QDRANT__SERVICE__API_KEY` in production

---

### pgvector

**Version-Specific Issues**:
- **<0.5.0**: Only IVFFlat index (slower than HNSW)
- **0.5.0+**: HNSW index available (much faster queries)
- **0.7.0+**: HNSW improvements, better recall

**Common Mistakes**:
1. **Building index before bulk insert**: Index build is 10x slower on large tables
2. **Not setting maintenance_work_mem**: Index builds fail or timeout
3. **Wrong operator**: Using `<->` (L2) with `vector_cosine_ops` index
4. **Forgetting ANALYZE**: Query planner doesn't know vector distribution

**Best Practices**:
- **Index timing**: Insert all data FIRST, then create index
- **Memory setting**: `SET maintenance_work_mem = '2GB'` before creating index
- **Operator matching**: `<=>` for Cosine, `<->` for L2, `<#>` for Dot Product
- **Post-insert**: Run `ANALYZE table_name` after bulk insert

---

### asyncpg

**Common Mistakes**:
1. **Using %s placeholders**: asyncpg uses $1, $2 (not psycopg2 %s syntax)
2. **Not using async with**: Connection leaks when `pool.acquire()` without context manager
3. **No ORDER BY in FOR UPDATE**: Causes deadlocks under concurrent load

**Best Practices**:
- **Placeholders**: Always use `$1, $2, $3` positional parameters
- **Context managers**: Always `async with pool.acquire() as conn`
- **Row locking**: Always add `ORDER BY id` to `SELECT ... FOR UPDATE`
- **Pool sizing**: min_size = CPU cores, max_size = 2-3x CPU cores

---

## Performance Gotchas

### 1. Hybrid Search Without Index Optimization

**Impact**: Latency (10x slower)
**Affects**: PostgreSQL ts_vector + pgvector hybrid search

**The problem**:
```sql
-- üíÄ SLOW - Combines two expensive operations without optimization
SELECT
    id,
    content,
    embedding <=> $1::vector AS vector_distance,
    ts_rank(search_vector, to_tsquery('english', $2)) AS text_rank
FROM document_chunks
WHERE search_vector @@ to_tsquery('english', $2)
ORDER BY (0.7 * (1 - (embedding <=> $1::vector)) + 0.3 * ts_rank(search_vector, to_tsquery('english', $2))) DESC
LIMIT 10;

-- Time: 850ms on 100K rows
```

**The solution**:
```sql
-- ‚úÖ FAST - Retrieve top candidates from each index, then combine
WITH vector_results AS (
    SELECT id, embedding <=> $1::vector AS distance
    FROM document_chunks
    ORDER BY embedding <=> $1::vector
    LIMIT 100  -- Top 100 from vector index
),
text_results AS (
    SELECT id, ts_rank(search_vector, to_tsquery('english', $2)) AS rank
    FROM document_chunks
    WHERE search_vector @@ to_tsquery('english', $2)
    ORDER BY rank DESC
    LIMIT 100  -- Top 100 from text index
)
SELECT DISTINCT
    c.id,
    c.content,
    COALESCE(v.distance, 2.0) AS vector_distance,
    COALESCE(t.rank, 0.0) AS text_rank,
    (0.7 * (1 - COALESCE(v.distance, 2.0) / 2) + 0.3 * COALESCE(t.rank, 0.0)) AS combined_score
FROM document_chunks c
LEFT JOIN vector_results v ON c.id = v.id
LEFT JOIN text_results t ON c.id = t.id
WHERE v.id IS NOT NULL OR t.id IS NOT NULL
ORDER BY combined_score DESC
LIMIT 10;

-- Time: 42ms on 100K rows (20x faster!)
```

**Benchmarks** (100K documents):

| Approach | Time | Speedup |
|----------|------|---------|
| Naive hybrid | 850ms | 1x |
| Separate indexes + merge | 42ms | 20x |
| Vector only | 12ms | 70x |
| Text only | 8ms | 106x |

---

### 2. Not Reusing Qdrant/OpenAI Clients

**Impact**: Latency (2-5x slower)
**Affects**: All API operations

**The problem**:
```python
# üíÄ SLOW - Creates new client for every request
@app.get("/search")
async def search(query: str):
    # New HTTP connection every time!
    client = AsyncQdrantClient("localhost")
    openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    # 30ms wasted on connection setup
    embedding = await openai_client.embeddings.create(...)
    results = await client.query_points(...)

    # Connections not closed!
    return results

# Time per request: ~80ms (30ms setup + 50ms actual work)
```

**The solution**:
```python
# ‚úÖ FAST - Reuse clients across requests
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Create clients once at startup
    app.state.qdrant_client = AsyncQdrantClient("localhost")
    app.state.openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    yield

    # Cleanup on shutdown
    app.state.qdrant_client.close()
    await app.state.openai_client.close()

app = FastAPI(lifespan=lifespan)

@app.get("/search")
async def search(query: str, request: Request):
    # Reuse existing clients (no connection overhead)
    embedding_response = await request.app.state.openai_client.embeddings.create(
        model="text-embedding-3-small",
        input=query
    )

    results = await request.app.state.qdrant_client.query_points(
        collection_name="documents",
        query=embedding_response.data[0].embedding,
        limit=10
    )

    return results

# Time per request: ~50ms (0ms setup + 50ms actual work)
# 1.6x faster!
```

---

### 3. Sequential Document Processing

**Impact**: Throughput (10x slower)
**Affects**: Document ingestion pipeline

**The problem**:
```python
# üíÄ SLOW - Process documents one at a time
async def ingest_documents(documents: list[Document]):
    for doc in documents:
        # 500ms per document
        parsed = await parse_document(doc)  # 100ms
        chunks = await chunk_document(parsed)  # 50ms
        embeddings = await generate_embeddings(chunks)  # 300ms
        await store_chunks(chunks, embeddings)  # 50ms

# 100 documents = 50 seconds
```

**The solution**:
```python
# ‚úÖ FAST - Process documents in parallel
async def ingest_documents_parallel(documents: list[Document], max_concurrent: int = 10):
    semaphore = asyncio.Semaphore(max_concurrent)

    async def process_one(doc: Document):
        async with semaphore:
            parsed = await parse_document(doc)
            chunks = await chunk_document(parsed)
            embeddings = await generate_embeddings(chunks)
            await store_chunks(chunks, embeddings)

    await asyncio.gather(*[process_one(doc) for doc in documents])

# 100 documents = 5 seconds (10x faster!)
```

---

## Security Gotchas

### 1. SQL Injection in Metadata Filters

**Severity**: Critical
**Type**: SQL Injection
**Affects**: Dynamic query building for metadata filters

**Vulnerability**:
```python
# üíÄ VULNERABLE - User input directly in SQL
async def search_documents(query: str, source_filter: str):
    sql = f"""
        SELECT * FROM documents
        WHERE search_vector @@ to_tsquery('english', '{query}')
        AND metadata->>'source' = '{source_filter}'
    """
    # If source_filter = "'; DROP TABLE documents; --"
    # Result: Your data is gone!

    rows = await conn.fetch(sql)
```

**Secure Implementation**:
```python
# ‚úÖ SECURE - Parameterized queries
async def search_documents(query: str, source_filter: str):
    # asyncpg escapes parameters automatically
    rows = await conn.fetch(
        """
        SELECT * FROM documents
        WHERE search_vector @@ to_tsquery('english', $1)
        AND metadata->>'source' = $2
        """,
        query,  # $1 - safely escaped
        source_filter  # $2 - safely escaped
    )
    return rows

# Why this works:
# 1. asyncpg validates and escapes all parameters
# 2. SQL structure is fixed (can't inject commands)
# 3. Even malicious input is treated as data, not code
```

**Testing for this vulnerability**:
```python
# Test with malicious input
malicious_input = "'; DROP TABLE documents; --"
results = await search_documents("test", malicious_input)

# With vulnerable code: Tables dropped üíÄ
# With secure code: Searches for literal string "'; DROP TABLE documents; --" ‚úÖ
```

---

### 2. Unauthenticated Qdrant Access

**Severity**: Critical
**Type**: Unauthorized Data Access
**Affects**: Qdrant vector database

**Vulnerability**:
```yaml
# üíÄ INSECURE - No authentication
services:
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"  # Open to the world!
    # No API key = anyone can read/write vectors
```

**Secure Implementation**:
```yaml
# ‚úÖ SECURE - API key required
services:
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    environment:
      - QDRANT__SERVICE__API_KEY=${QDRANT_API_KEY}
    networks:
      - internal  # Not exposed to internet

networks:
  internal:
    driver: bridge
```

```python
# Application uses API key
from qdrant_client import AsyncQdrantClient

client = AsyncQdrantClient(
    url="http://qdrant:6333",
    api_key=os.getenv("QDRANT_API_KEY")  # Required
)
```

---

## Testing Gotchas

**Common Test Pitfalls**:
1. **Not cleaning up test data**: Vector collections accumulate garbage across tests
2. **Using production OpenAI key**: Racks up costs during testing
3. **Not mocking slow operations**: Tests take minutes instead of seconds
4. **Hardcoded embedding dimensions**: Tests break when changing models

**Better test pattern**:
```python
import pytest
import pytest_asyncio
from unittest.mock import AsyncMock, patch

@pytest_asyncio.fixture
async def test_qdrant():
    """Ephemeral Qdrant collection for testing."""
    from qdrant_client import AsyncQdrantClient

    client = AsyncQdrantClient(":memory:")  # In-memory for tests
    collection_name = f"test_{uuid.uuid4()}"

    await client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=384, distance=Distance.COSINE)
    )

    yield client, collection_name

    # Cleanup
    await client.delete_collection(collection_name)

@pytest.mark.asyncio
async def test_document_search(test_qdrant):
    client, collection = test_qdrant

    # Mock OpenAI to avoid costs
    with patch("openai.AsyncOpenAI") as mock_openai:
        mock_openai.return_value.embeddings.create = AsyncMock(
            return_value=Mock(data=[Mock(embedding=[0.1] * 384)])
        )

        # Test with mocked embedding
        service = SearchService(client, collection)
        results = await service.search("test query")

        assert len(results) >= 0
```

---

## Deployment Gotchas

**Environment-Specific Issues**:

- **Development**:
  - Gotcha: Using `:latest` tags (breaks reproducibility)
  - Fix: Pin versions in docker-compose.yml

- **Production**:
  - Gotcha: Insufficient memory limits cause OOMKilled
  - Fix: Set explicit `mem_limit` in docker-compose

**Configuration Issues**:
```yaml
# ‚ùå WRONG - Secrets in docker-compose.yml
services:
  api:
    environment:
      - OPENAI_API_KEY=sk-abc123...  # üíÄ Committed to Git!

# ‚úÖ RIGHT - Secrets from .env file
services:
  api:
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}  # Read from .env
```

---

## Anti-Patterns to Avoid

### 1. Storing Zero Embeddings on Failure

**What it is**: Storing `[0.0, 0.0, ...]` when embedding generation fails

**Why it's bad**: Zero vectors match everything with equal (ir)relevance, corrupting search

**Better pattern**: Skip failed items, track for retry (see Gotcha #1)

---

### 2. Not Truncating MCP Responses

**What it is**: Returning 50KB+ content through MCP tools

**Why it's bad**: Exceeds AI assistant token limits, causes poor UX

**Better pattern**: Truncate to 1000 chars for lists, full content for single get (see Gotcha #11)

---

### 3. Manual tsvector Updates

**What it is**: Manually setting `search_vector` column on INSERT/UPDATE

**Why it's bad**: Easy to forget, leads to stale search indexes

**Better pattern**: Use trigger for automatic updates (see Gotcha #12)

---

## Gotcha Checklist for Implementation

Before marking PRP complete, verify these gotchas are addressed:

### Critical Issues
- [ ] **Embedding failures**: Never store zero/null embeddings (Gotcha #1)
- [ ] **Connection pool deadlock**: Share pool, not connections (Gotcha #2)
- [ ] **SQL injection**: Use $1, $2 placeholders, never string formatting (Gotcha #3)
- [ ] **Database deadlocks**: Always ORDER BY id in SELECT FOR UPDATE (Gotcha #4)

### High Priority
- [ ] **Vector dimensions**: Validate dimensions match collection config (Gotcha #5)
- [ ] **pgvector indexes**: Verify EXPLAIN shows Index Scan, not Seq Scan (Gotcha #6)
- [ ] **OpenAI rate limits**: Batch requests, implement exponential backoff (Gotcha #7)
- [ ] **Chunking strategy**: Use semantic chunking with overlap (Gotcha #8)
- [ ] **pgvector dimension limit**: Keep dimensions ‚â§ 2000 or use Qdrant (Gotcha #9)
- [ ] **Qdrant memory**: Estimate memory needs, enable quantization (Gotcha #10)

### Medium Priority
- [ ] **MCP response size**: Truncate large fields to 1000 chars (Gotcha #11)
- [ ] **tsvector updates**: Use trigger for automatic updates (Gotcha #12)
- [ ] **crawl4ai setup**: Install browser binaries with crawl4ai-setup (Gotcha #13)
- [ ] **Qdrant distance metric**: Normalize vectors for Cosine distance (Gotcha #14)

### Security
- [ ] **SQL injection prevention**: All queries use parameterization
- [ ] **Qdrant authentication**: API key required in production
- [ ] **Secret management**: No secrets in docker-compose.yml or code

### Performance
- [ ] **Hybrid search optimization**: Use separate index queries + merge
- [ ] **Client reuse**: Qdrant/OpenAI clients created once at startup
- [ ] **Parallel processing**: Document ingestion uses concurrency

---

## Sources Referenced

### From Archon Knowledge Base
- No specific RAG gotcha documents found in Archon (searches returned general content)

### From Web Research
- **Qdrant pitfalls**: https://dagshub.com/blog/common-pitfalls-to-avoid-when-using-vector-databases/
- **OpenAI rate limits**: https://platform.openai.com/docs/guides/rate-limits
- **OpenAI rate limit community**: https://community.openai.com/t/rate-limit-reached-with-large-documents/358525
- **pgvector performance**: https://www.crunchydata.com/blog/pgvector-performance-for-developers
- **pgvector performance (Microsoft)**: https://learn.microsoft.com/en-us/azure/cosmos-db/postgresql/howto-optimize-performance-pgvector
- **pgvector indexing**: https://www.cybertec-postgresql.com/en/indexing-vectors-in-postgresql/
- **RAG chunking strategies**: https://www.f22labs.com/blogs/7-chunking-strategies-in-rag-you-need-to-know/
- **Chunking best practices**: https://unstructured.io/blog/chunking-for-rag-best-practices
- **asyncpg connection pool**: https://usamabjw.medium.com/improving-latency-of-database-calls-in-fastapi-with-asyncio-lifespan-events-and-connection-818066db59ab
- **FastAPI connection pool deadlock**: https://github.com/nsidnev/fastapi-realworld-example-app/issues/64

### From Codebase
- Task-manager gotchas (codebase-patterns.md): Connection pool, SQL placeholders, row locking, MCP response size
- Task-manager service patterns (codebase-patterns.md): Service layer, error handling, pagination

---

## Recommendations for PRP Assembly

When generating the PRP:

1. **Include critical gotchas in "Known Gotchas & Library Quirks" section**
   - Gotcha #1-4 (Critical): Must address in implementation
   - Gotcha #5-10 (High): Should address, provide solutions
   - Gotcha #11-14 (Medium): Nice to have, document workarounds

2. **Reference solutions in "Implementation Blueprint"**
   - Connection pool management (lifespan pattern)
   - Batch embedding generation with retry
   - Semantic chunking implementation
   - Hybrid search query optimization

3. **Add detection tests to validation gates**
   - Test: Verify no zero embeddings stored
   - Test: Verify pgvector indexes used (EXPLAIN)
   - Test: Verify MCP responses <50KB
   - Test: Load test connection pool (no deadlock)

4. **Warn about version issues in documentation references**
   - pgvector: Recommend 0.7.0+ for HNSW
   - Qdrant: Require 1.6.0+ for async client
   - asyncpg: Use $1, $2 placeholders (not %s)

5. **Highlight anti-patterns to avoid**
   - Never store zero embeddings
   - Never share connections (share pool)
   - Never manual tsvector updates (use triggers)
   - Never return full content via MCP (truncate)

## Confidence Assessment

**Gotcha Coverage**: 8/10
- **Security**: High confidence (SQL injection, auth, secrets covered)
- **Performance**: High confidence (indexing, pooling, batching covered)
- **Common Mistakes**: High confidence (web research + codebase analysis)
- **Vector DB specifics**: Medium-high confidence (Qdrant/pgvector docs + community issues)
- **Library quirks**: High confidence (asyncpg, OpenAI, crawl4ai documented)

**Gaps**:
- **Docling-specific gotchas**: Limited documentation on edge cases
- **crawl4ai rate limiting**: Not well documented (sites may block)
- **Reranking gotchas**: CrossEncoder issues (post-MVP, lower priority)
- **Multi-tenancy**: Not covered (out of scope for MVP)

**Total Gotchas Documented**: 16 major + numerous minor issues
**Solutions Provided**: 100% (every gotcha has solution with code)
**Real-world Sources**: Web research + codebase patterns + official docs
